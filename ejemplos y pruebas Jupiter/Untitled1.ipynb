{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6f826f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://L2203083.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.0.3, master = local[*], app id = local-1651046342112)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//ejercicio sencillo para empezar con spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66fc7c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "strings: org.apache.spark.sql.DataFrame = [value: string]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val strings = spark.read.text(\"C:/Spark/spark-3.0.3-bin-hadoop2.7/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d148ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      # Apache Spark|\n",
      "|                    |\n",
      "|Spark is a unifie...|\n",
      "|high-level APIs i...|\n",
      "|supports general ...|\n",
      "|rich set of highe...|\n",
      "|MLlib for machine...|\n",
      "|and Structured St...|\n",
      "|                    |\n",
      "|<https://spark.ap...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "strings.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c57a457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 108\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1e123da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prueba: org.apache.spark.sql.DataFrame = [value: string]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prueba = spark.read.text(\"C:/Users/marcos.corona/Documents/GitHub/LearningSparkV2Cap2/ejemploSencillo1.scala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a19aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|// Databricks not...|\n",
      "|val strings = spa...|\n",
      "|                    |\n",
      "|// COMMAND ------...|\n",
      "|                    |\n",
      "|      strings.show()|\n",
      "|                    |\n",
      "|// COMMAND ------...|\n",
      "|                    |\n",
      "|//HAY QUE TENER E...|\n",
      "|                    |\n",
      "|// COMMAND ------...|\n",
      "|                    |\n",
      "|                    |\n",
      "+--------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "prueba.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a78df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "042f160b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: string]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtered = strings.filter(col(\"value\").contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e6fef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 19\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7404ea84",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "26: error: reassignment to val\r",
     "output_type": "error",
     "traceback": [
      "<console>:26: error: reassignment to val\r",
      "           spark = SparkSession\r",
      "                 ^\r",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "object MnMcount {\n",
    "def main(args: Array[String]) {\n",
    "spark = SparkSession\n",
    " .builder\n",
    " .appName(\"MnMCount\")\n",
    " .getOrCreate()\n",
    " if (args.length < 1) {\n",
    " print(\"Usage: MnMcount <mnm_file_dataset>\")\n",
    " sys.exit(1)\n",
    " }\n",
    " // Get the M&M data set filename\n",
    " val mnmFile = args(0)\n",
    "// Read the file into a Spark DataFrame\n",
    " val mnmDF = spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .load(mnmFile)\n",
    " // Aggregate counts of all colors and groupBy() State and Color\n",
    " // orderBy() in descending order\n",
    " val countMnMDF = mnmDF\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(count(\"Count\").alias(\"Total\"))\n",
    " .orderBy(desc(\"Total\"))\n",
    " // Show the resulting aggregations for all the states and colors\n",
    " countMnMDF.show(60)\n",
    " println(s\"Total Rows = ${countMnMDF.count()}\")\n",
    " println()\n",
    " // Find the aggregate counts for California by filtering\n",
    " val caCountMnMDF = mnmDF\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .where(col(\"State\") === \"CA\")\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(count(\"Count\").alias(\"Total\"))\n",
    " .orderBy(desc(\"Total\"))\n",
    " // Show the resulting aggregations for California\n",
    " caCountMnMDF.show(10)\n",
    " // Stop the SparkSession\n",
    " spark.stop()\n",
    "}\n",
    "}    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5763eadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: illegal start of definition\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: illegal start of definition\r",
      "       package main.scala.chapter2\r",
      "       ^\r",
      ""
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74c010de",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: ';' expected but 'package' found.\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: ';' expected but 'package' found.\r",
      "       build/sbt clean package;\r",
      "                       ^\r",
      ""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f6f2997",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "34: error: not found: value name\r",
     "output_type": "error",
     "traceback": [
      "<console>:34: error: not found: value name\r",
      "       name := \"main/scala/chapter2\"\r",
      "       ^\r",
      ""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34a510f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: illegal start of definition\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: illegal start of definition\r",
      "       package main.scala.chapter2\r",
      "       ^\r",
      ""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53307e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
